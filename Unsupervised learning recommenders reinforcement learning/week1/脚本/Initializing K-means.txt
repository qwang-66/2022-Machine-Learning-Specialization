The very first step of the K means
clustering algorithm, was to choose random locations as the initial guesses for
the cluster centroids mu one through mu K. But how do you actually
take that random guess. Let's take a look at that in this video,
as well as how you can take multiple attempts at the initial guesses
with mu one through mu K. That will result in your finding
a better set of clusters. Let's take a look,
here again is the K means algorithm and in this video let's take a look at how
you can implement this first step. When running K means you should pretty
much always choose the number of cluster central's K to be
lessened to training examples m. It doesn't really make sense to have
K greater than m because then there won't even be enough training examples
to have at least one training example per cluster centroids. So in our earlier example we had
K equals two and m equals 30. In order to choose the cluster centroids, the most common way is to randomly
pick K training examples. So here is a training set where if I were
to randomly pick two training examples, maybe I end up picking this one and
this one. And then we would set new one through mu
K equal to these K training examples. So I might initialize my
red cluster centroid here, and initialize my blue
cluster sent troy over here, in the example where K was equal to two. And it turns out that if this was
your random initialization and you were to run K means you
pray end up with K means deciding that these are the two
classes in the data set. Notes that this method of initializing the
cost of central is a little bit different than what I had used in
the illustration in the earlier videos. Where I was initializing the cluster
centroids mu one and mu two to be just random points rather than sitting on
top of specific training examples. I've done that to make the illustrations
clearer in the earlier videos. But what I'm showing in this slide is
actually a much more commonly used way of initializing the clusters centroids. Now with this method there is a chance
that you end up with an initialization of the cluster centroids where the red cross
is here and maybe the blue cross is here. And depending on how you choose
the random initial central centroids kineys\g raw end up picking a difference
set of causes for your data set. Let's look at a slightly more complex
example, where we're going to look at this data set and try to find three
clusters so k equals three in this data. If you were to run K means with one random
initialization of the cluster centroid, you may get this result up here and
this looks like a pretty good choice. Pretty good clustering of the data
into three different clusters. But with a different initialization,
say you had happened to initialize two of the cluster centroids
within this group of points. And one within this group of points,
after running k means you might end up with this clustering,
which doesn't look as good. And this turns out to be a local optima,
in which K-means is trying to minimize the distortion cost function,
that cost function J of C one through CM and mu one through
mu K that you saw in the last video. But with this less fortunate
choice of random initialization, it had just happened to get
stuck in a local minimum. And here's another example
of a local minimum, where a different random initialization
course came in to find this clustering of the data
into three clusters, which again doesn't seem as good as
the one that you saw up here on top. So if you want to give k means multiple
shots at finding the best local optimum. If you want to try multiple
random initialization, so give it a better chance of finding
this good clustering up on top. One other thing you could
do with the kings algorithm is to run it multiple times and
then to try to find the best local optima. And it turns out that if you were
to run k means three times say, and end up with these three
distinct clusterings. Then one way to choose between
these three solutions, is to compute the cost function J for
all three of these solutions, all three of these choices of
clusters found by k means. And then to pick one of these
three according to which one of them gives you the lowest value for
the cost function J. And in fact, if you look at this
grouping of clusters up here, this green cross has relatively small
square distances, all the green dots. The red cross is relatively small distance
and red dots and similarly the blue cross. And so the cost function J will be
relatively small for this example on top. But here, the blue cross has larger
distances to all of the blue dots. And here the red cross has larger
distances to all of the red dots, which is why the cost function J, for
these examples down below would be larger. Which is why if you pick
from these three options, the one with the smallest distortion
of the smallest cost function J. You end up selecting this choice
of the cluster centroids. So let me write this out more formally
into an algorithm, and wish you would ranking means multiple times using
different random initialization. Here's the algorithm, if you want to
use 100 random initialization for K-means, then you would run
100 times randomly initialized K-means using the method that
you saw earlier in this video. Pick K training examples and
let the cluster centuries initially be the locations of
those K training examples. Using that random initialization,
run the K-means algorithm to convergence. And that will give you a choice of cluster
assignments and cluster centroids. And then finally, you would compute the distortion
compute the cost function as follows. After doing this, say 100 times, you would finally pick the set of
clusters, that gave the lowest cost. And it turns out that if you do
this will often give you a much better set of clusters,
with a much lower distortion function than if you were to
run K means only a single time. I plugged in the number up here as 100. When I'm using this method, doing this somewhere between say 50
to 1000 times would be pretty common. Where, if you run this procedure
a lot more than 1000 times, it tends to get computational expensive. And you tend to have diminishing
returns when you run it a lot of times. Whereas trying at least maybe 50 or 100 random initializations,
will often give you a much better result than if you only had one shot at
picking a good random initialization. But with this technique you are much more
likely to end up with this good choice of clusters on top. And these less superior local
minima down at the bottom. So that's it, when I'm using
the K means algorithm myself, I will almost always use more
than one random initialization. Because it just causes K means to do a
much better job minimizing the distortion cost function and finding a much better
choice for the cost of centroids. Before we wrap up our
discussion of K means, there's just one more video in
which I hope to discuss with you. The question of how do you choose
the number of clusters centroids? How do you choose the value of K? Let's go on to the next video
to take a look at that.